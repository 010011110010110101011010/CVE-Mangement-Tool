from django.shortcuts import render, get_object_or_404, redirect
from django.db import connection
from django.core.paginator import Paginator
from django.shortcuts import render
from django.db import connections
from django.http import HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
from .models import CVE
import requests
from django.urls import reverse
import pymysql
import os
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from django.shortcuts import render, redirect
from .models import Software
from django.urls import reverse_lazy
from django.views.generic import ListView, CreateView, UpdateView, DeleteView
from .models import Software

# global today_date

base_url = 'https://www.dgssi.gov.ma'

french_to_english_months = {
    "Janvier": "January", "Février": "February", "Mars": "March", "Avril": "April",
    "Mai": "May", "Juin": "June", "Juillet": "July", "Août": "August",
    "Septembre": "September", "Octobre": "October", "Novembre": "November", "Décembre": "December",
    "janv": "January", "févr": "February", "mars": "March", "avril": "April",
    "mai": "May", "juin": "June", "juil": "July", "août": "August",
    "sept": "September", "oct": "October", "nov": "November", "déc": "December",
    "janvier": "January", "février": "February", "mars": "March", "avril": "April",
    "mai": "May", "juin": "June", "juillet": "July", "août": "August",
    "septembre": "September", "octobre": "October", "novembre": "November", "décembre": "December"
}

db_host = "localhost"
db_user = "root"
db_password = os.environ.get('EXAPWD', 'exasol'),
db_name = "CV"

def data_table(request):
    formatted_data = []
    try:
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT title, publication_date, reference_number, risk_level, impact_level, 
            affected_systems, external_identifiers, summary, solution, risk, reference, pdf
                FROM CV.cves ORDER BY publication_date DESC
            """)
            data_list = cursor.fetchall()

            for row in data_list:
                formatted_data.append({
                    'title': row[0],
                    'publication_date': row[1],
                    'reference_number': row[2],
                    'risk_level': row[3],
                    'impact_level': row[4],
                    'affected_systems': row[5],
                    'external_identifiers': row[6],
                    'summary': row[7],
                    'solution': row[8],
                    'risk': row[9],
                    'reference': row[10],
                    'pdf': row[11],
                })

    except Exception as e:
        print(f"Error fetching data: {e}")

    return render(request, 'data_table.html', {'data_list': formatted_data})

'''def First_View(request):
    formatted_data = []
    try:
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT Titre, DatePub, NumRef
                FROM CV.cves ORDER BY STR_TO_DATE(DatePub, '%Y-%m-%d') DESC
            """)
            data_list = cursor.fetchall()

            for row in data_list:
                formatted_data.append({
                    'Titre': row[0],
                    'DatePub': row[1],
                    'NumRef': row[2],
                })

    except Exception as e:
        print(f"Error fetching data: {e}")

    # Pagination
    paginator = Paginator(formatted_data, 10)  # Show 10 items per page
    page_number = request.GET.get('page')
    page_obj = paginator.get_page(page_number)

    return render(request, 'First_View.html', {'data_list1': page_obj})'''

def dashboard_view(request):
    os_data = []  # List to store data for each OS

    try:
        with connection.cursor() as cursor:
            cursor.execute("SELECT * FROM CV.operatingsystems")
            os_list = [row[0] for row in cursor.fetchall()]

            for os_name in os_list:
                query = f"""
                    SELECT COUNT(*) FROM CV.cves 
                    WHERE (affected_systems LIKE '%{os_name}%' OR title LIKE '%{os_name}%') AND status IN ('Non-patched', 'Directed')
                """
                
                cursor.execute(query)
                cve_count = cursor.fetchone()[0]

                os_data.append({
                    'os_name': os_name,
                    'cve_count': cve_count,
                })

    except Exception as e:
        print(f"Error fetching data: {e}")

    return render(request, 'dashboard.html', {'os_data': os_data})

def First_View(request):
    os_data = []  # List to store data for each OS

    try:
        with connection.cursor() as cursor:
            # Fetch OS names used in the company
            cursor.execute("SELECT * FROM CV.operatingsystems")
            os_list = [row[0] for row in cursor.fetchall()]

            # Iterate over each OS name
            for os_name in os_list:
                # Construct the query with a LIKE condition for the current OS
                query = f"""
                    SELECT title, publication_date, reference_number
                    FROM CV.cves 
                    WHERE affected_systems LIKE '%{os_name}%'
                    ORDER BY publication_date DESC
                """
                
                cursor.execute(query)
                data_list = cursor.fetchall()
                
                # Append data for the current OS to os_data
                os_data.append({
                    'os_name': os_name,
                    'articles': [
                        {
                            'title': row[0],
                            'publication_date': row[1],
                            'reference_number': row[2]
                        } for row in data_list
                    ]
                })

    except Exception as e:
        print(f"Error fetching data: {e}")
    
    os_data = [entry for entry in os_data if entry['articles']]

    # Use Django's Paginator to handle pagination
    from django.core.paginator import Paginator

    # Pagination
    paginator = Paginator(os_data, 2)  # 2 tables per page
    page_number = request.GET.get('page') or 1  # Default to the first page
    page_obj = paginator.get_page(page_number)

    return render(request, 'First_View.html', {'page_obj': page_obj, 'os_data': os_data})

def main_page(request):
    return render(request, 'index.html')

def index(request):
    return render(request, 'index.html')

def data_view(request):
    item_id = request.GET.get('item_id')
    item_data = {}
        
    if item_id:
        try:
            with connection.cursor() as cursor:
                cursor.execute("""
                    SELECT title, status, publication_date, reference_number, risk_level, impact_level, affected_systems, external_identifiers, summary, solution, risk, reference, pdf 
                    FROM CV.cves 
                    WHERE reference_number = %s
                """, [item_id])
                row = cursor.fetchone()

                if row:
                # Convert affected_systems to a bullet point list
                    affected_systems = row[6]
                    affected_systems_list = affected_systems.split(',')
                    formatted_affected_systems = ''.join(f"<li>{system.strip()}</li>" for system in affected_systems_list)

                    # Wrapping the items in <ul> tag
                    formatted_affected_systems = f"<ul>{formatted_affected_systems}</ul>"

                    external_identifiers = row[7]
                    external_identifiers_list = external_identifiers.split(',')
                    formatted_external_identifiers = ''.join(f"<li>{system.strip()}</li>" for system in external_identifiers_list)

                    # Wrapping the items in <ul> tag
                    formatted_external_identifiers = f"<ul>{formatted_external_identifiers}</ul>"
                
                    risk = row[10]
                    risk_list = risk.split(',')
                    risk_identifiers = ''.join(f"<li>{system.strip()}</li>" for system in risk_list)

                    # Wrapping the items in <ul> tag
                    risk_identifiers = f"<ul>{risk_identifiers}</ul>"

                    reference = row[11]  # Assuming this is a comma-separated string of URLs
                    reference_list = reference.split(',')

                    # Format each item as a clickable link and wrap it in a <li> tag
                    reference_identifiers = ''.join(
                        f"<li><a href='{system.strip()}' target='_blank'  style='text-decoration: underline;'>{system.strip()}</a></li>" for system in reference_list
                    )

                    # Wrap all <li> items in a <ul> tag
                    reference_identifiers = f"<ul>{reference_identifiers}</ul>"

                    item_data = {
                        'title': row[0],
                        'status': row[1],
                        'publication_date': row[2],
                        'reference_number': row[3],
                        'risk_level': row[4],
                        'impact_level': row[5],
                        'affected_systems': formatted_affected_systems,
                        'external_identifiers': formatted_external_identifiers,
                        'summary': row[8],
                        'solution': row[9],
                        'risk': risk_identifiers,
                        'reference': reference_identifiers,
                        'pdf': row[12],
                    }
                    return render(request, 'data_detail.html', {'item': item_data})
                else:
                    return render(request, 'data_detail.html', {'item': item_data})

        except Exception as e:
            print(f"Error fetching data: {e}")
            return render(request, 'data_detail.html', {'item': item_data})

    else:
        return render(request, 'data_detail.html', {'item': item_data})

def refresh_data(request):
    # Call your Refresh function
    urls=get_urls()
    
    failed_urls = []
    
    for url in urls:
        result = extract_data_from_page(url)
        if not result:
            failed_urls.append(url)
        else:
            print(f"Success: Data extracted for {url}")
            insert_data_into_db(result)
    
    if failed_urls:
        with open("errors.txt", 'w') as file:
            for url in failed_urls:
                file.write(f"{url}\n")
        print(f"Failed URLs written back to errors.txt: {len(failed_urls)}")
    else:
        print("All URLs processed successfully.")
    
    RemoveDupl()
    return redirect('First_View')

def get_connection():
    return pymysql.connect(
        host='localhost',
        user='root',
        password = os.environ.get('EXAPWD', 'exasol'),
        db='CV'
    )

def is_url_crawled(url):
    """Check if a specific URL has been crawled and its visited status."""
    connection = get_connection()
    try:
        with connection.cursor() as cursor:
            sql = "SELECT visited FROM crawled_urls WHERE url = %s"
            cursor.execute(sql, (url,))
            result = cursor.fetchone()
            if result is not None:
                return result[0]  # Return the visited status (True/False)
            else:
                return None  # URL not found
    finally:
        connection.close()

def is_url(url):
    """Check if a specific URL has been crawled and its visited status."""
    connection = get_connection()
    try:
        with connection.cursor() as cursor:
            sql = "SELECT url FROM crawled_urls WHERE url = %s"
            cursor.execute(sql, (url))
            result = cursor.fetchone()
            if result is not None:
                return True
              # Return the visited status (True/False)
            else:
                return None  # URL not found
    finally:
        connection.close()

def insert_data_into_db(data):
    try:
        # Establish the database connection
        conn = get_connection()
        cursor = conn.cursor()

        # Prepare the SQL query
        insert_query = """
        INSERT INTO cves (
            title, publication_date, reference_number, risk_level, impact_level, 
            affected_systems, external_identifiers, summary, solution, risk, reference, pdf
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
        """

        # Execute the query with the data
        cursor.execute(insert_query, (
            data[0],  # title
            data[1],  # publication_date
            data[2],  # reference_number
            data[3],  # risk_level
            data[4],  # impact_level
            ','.join(data[5]),  # affected_systems (stored as CSV)
            ','.join(data[6]),  # external_identifiers (stored as CSV)
            data[7],  # summary
            data[8],  # solution
            ','.join(data[9]),  # risk (stored as CSV)
            ','.join(data[10]),  # references (stored as CSV)
            data[11]  # pdf
        ))

        # Commit the transaction
        conn.commit()

        print("Data inserted successfully")

    except Exception as error:
        print(f"Failed to insert data into database: {error}")

    finally:
        # Close the cursor and connection
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def add_crawled_url(url, pdf_url):
    """Add a new URL to the crawled_urls table if it doesn't already exist."""
    connection = get_connection()
    try:
        with connection.cursor() as cursor:
            # Check if the URL already exists
            check_sql = "SELECT id FROM crawled_urls WHERE url = %s"
            cursor.execute(check_sql, (url,))
            result = cursor.fetchone()
            
            if result is None:
                # URL does not exist, so insert it
                insert_sql = "INSERT INTO crawled_urls (url, pdf_url, visited) VALUES (%s, %s, FALSE)"
                cursor.execute(insert_sql, (url, pdf_url))
                connection.commit()
                print(f"URL added: {url}")
                return True
            else:
                print(f"URL already exists: {url}")
                return False
    finally:
        connection.close()

def get_urls():
    base_url = 'https://www.dgssi.gov.ma'
    """Fetch bulletin links from the site for today's date and return a map of bulletin links to their PDF URL."""
    page_url = f'{base_url}/index.php/fr/bulletins-securite'
    
    # bulletin_pdf_map = {}  # Dictionary to store bulletin link and its PDF URL

    response = requests.get(page_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')

        bulletin_links = set()
        links = soup.find_all('a', href=True)

        for link in links:
            href = link['href']
            if '/index.php/fr/bulletins/' in href:
                full_url = href if href.startswith('http') else urljoin(base_url, href)
                # print(f"Adding URL to process: {full_url}")
                bulletin_links.add(full_url)

    else:
        print(f"Failed to fetch page: {response.status_code}")

    return bulletin_links

def parse_french_date(date_str):
    """Convert a French date string to a YYYY-MM-DD date string."""
    day, month_str, year = date_str.strip().split()

    month = french_to_english_months.get(month_str)
    if not month:
        print(f"Unknown French month name: {month_str}")
        return None

    try:
        date_obj = datetime.strptime(f"{day} {month} {year}", '%d %B %Y')
        # Return the date in YYYY-MM-DD format
        return date_obj.strftime('%Y-%m-%d')  # Example: '2024-08-27'
    except ValueError as e:
        print(f"Date parsing failed for date string: {date_str}")
        print(f"Error: {e}")
        return None

def extract_data_from_page(url):
    # Ensure the URL is valid
    if not url.startswith(('http://', 'https://')):
        raise ValueError("The URL must start with 'http://' or 'https://'")

    # Fetch the HTML content
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise HTTPError for bad responses
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the URL: {e}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')

    # Extracting details
    try:
        title = soup.find('h1', class_='main-title').get_text(strip=True) if soup.find('h1', class_='main-title') else "N/A"
        publication_date = soup.find('div', class_='field--name-field-date').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-field-date') else "N/A"
        publication_date = parse_french_date(publication_date)
        reference_number = soup.find('div', class_='field--name-field-numero-de-reference').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-field-numero-de-reference') else "N/A"
        risk_level = soup.find('div', class_='field--name-field-niveau-de-risque').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-field-niveau-de-risque') else "N/A"
        impact_level = soup.find('div', class_='field--name-field-niveau-d-impact').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-field-niveau-d-impact') else "N/A"

        # Extracting Affected Systems
        systems = soup.find('div', class_='field--name-field-systemes-affectes').find('div', class_='field__item') if soup.find('div', class_='field--name-field-systemes-affectes') else None
        affected_systems = [li.get_text(strip=True) for li in systems.find_all('li')] if systems else []

        # Extracting External Identifiers
        identifiers = soup.find('div', class_='field--name-field-identificateurs-externes').find('div', class_='field__item') if soup.find('div', class_='field--name-field-identificateurs-externes') else None
        external_identifiers = [li.get_text(strip=True) for li in identifiers.find_all('li')] if identifiers else []

        # Extracting Vulnerability Summary
        summary = soup.find('div', class_='field--name-body').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-body') else "N/A"

        # Extracting Solution
        solution = soup.find('div', class_='field--name-field-solution').find('div', class_='field__item').get_text(strip=True) if soup.find('div', class_='field--name-field-solution') else "N/A"

        # Extracting Risk
        risk = [li.get_text(strip=True) for li in soup.find('div', class_='field--name-field-risque').find('div', class_='field__item').find_all('li')] if soup.find('div', class_='field--name-field-risque') else []

        # Extracting References
        references = [a.get('href') for a in soup.find('div', class_='field--name-field-reference').find('div', class_='field__item').find_all('a', href=True)] if soup.find('div', class_='field--name-field-reference') else []

        # Extracting PDF Link
        pdf_link_tag = soup.find('a', href=True, target="_blink")
        pdf_link_relative = pdf_link_tag['href'] if pdf_link_tag else None

        # Prepend the base URL to the relative URL
        base_url = 'https://www.dgssi.gov.ma'  # Replace with the actual base URL
        pdf = urljoin(base_url, pdf_link_relative) if pdf_link_relative else None

        return (title, publication_date, reference_number, risk_level, impact_level, affected_systems, external_identifiers, summary, solution, risk, references, pdf)
    except AttributeError as e:
        return None

def fetch_urls():
    # Define the query to fetch URLs
    query = "SELECT url FROM crawled_urls"

    # Establish a connection to the database
    connection = get_connection()
    try:
        with connection.cursor() as cursor:
            # Execute the query
            cursor.execute(query)
            # Fetch all results
            results = cursor.fetchall()
            # Create a list to store URLs
            urls = []
            # Add each URL and PDF URL to the list
            urls = [row[0] for row in results]
            return urls
    finally:
        # Close the connection
        connection.close()
####################################################################################################################################################################################
####################################################################################################################################################################################
def cve_statistics(request):
    Patched_cves = CVE.objects.filter(status='Patched').count()
    NonPatched_cves = CVE.objects.filter(status='Non-Patched').count()
    directed_cves = CVE.objects.filter(status='Directed').count()


    return JsonResponse({
        'Patched': Patched_cves,  # Number of mitigated CVEs
        'NonPatched': NonPatched_cves,  # Number of live (Non-Patched) CVEs
        'Directed': directed_cves,  # Number of Directed CVEs
    })

def recent_cves(request):
    cves = CVE.objects.all().order_by('-date_pub')[:10]  # Example: Get the 10 most recent CVEs
    cve_data = [
        {
            'id': cve.titre,          # CVE ID
            'description': cve.risque,   # Description
            'severity': cve.impact,       # Severity
            'date': cve.date_pub,
            'reference_number': cve.num_ref
        }
        for cve in cves
    ]
    return JsonResponse(cve_data, safe=False)

def os_details(request, os_name):       
    if request.method == 'GET':

        os_data = []  # List to store data for the specific OS

        try:
            with connection.cursor() as cursor:
                # Fetch the CVEs related to the specific OS
                query = f"""
                    SELECT title, publication_date, reference_number, status
                    FROM CV.cves 
                    WHERE (affected_systems LIKE '%{os_name}%' OR title LIKE '%{os_name}%')
                    AND status NOT LIKE 'Patched'
                    ORDER BY publication_date DESC;
                """
                cursor.execute(query)
                data_list = cursor.fetchall()

                # Append the data for the specific OS
                os_data.append({
                    'os_name': os_name,
                    'articles': [
                        {
                            'title': row[0],
                            'publication_date': row[1],
                            'reference_number': row[2],
                            'status': row[3],
                        } for row in data_list
                    ]
                })

        except Exception as e:
            print(f"Error fetching data: {e}")

        # Ensure only OS with articles are passed
        os_data = [entry for entry in os_data if entry['articles']]

        # Use Django's Paginator to handle pagination
        paginator = Paginator(os_data, 2)  # 2 articles per page
        page_number = request.GET.get('page') or 1  # Default to the first page
        page_obj = paginator.get_page(page_number)

    return render(request, 'os_details.html', {'page_obj': page_obj, 'os_data': os_data})

def update_status(request):
    if request.method == 'POST':
        reference_number = request.POST.get('reference_number')
        os_name = request.POST.get('os_name')
        selected_status = request.POST.get('status')
        print(reference_number, os_name, selected_status)

        status_to_update = selected_status
        
        try:
            with connection.cursor() as cursor:
                query = f"""
                    UPDATE CV.cves 
                    SET status = %s
                    WHERE reference_number = %s
                """
                cursor.execute(query, [status_to_update, reference_number])
            
            status_message = 'Status updated successfully!'
        except Exception as e:
            status_message = f"Error updating data: {e}"

        return redirect(f"{reverse('os_details', args=[os_name])}?status_message={status_message}")

    return redirect(reverse('os_details', args=[os_name]))

def All(request, os_name):
    if request.method == 'GET':

        os_data = []  # List to store data for the specific OS

        try:
            with connection.cursor() as cursor:
                # Fetch the CVEs related to the specific OS
                query = f"""
                    SELECT title, publication_date, reference_number, status
                    FROM CV.cves 
                    WHERE affected_systems LIKE '%{os_name}%'
                    ORDER BY publication_date DESC
                """
                cursor.execute(query)
                data_list = cursor.fetchall()

                # Append the data for the specific OS
                os_data.append({
                    'os_name': os_name,
                    'articles': [
                        {
                            'title': row[0],
                            'publication_date': row[1],
                            'reference_number': row[2],
                            'status': row[3],
                        } for row in data_list
                    ]
                })

        except Exception as e:
            print(f"Error fetching data: {e}")

        # Ensure only OS with articles are passed
        os_data = [entry for entry in os_data if entry['articles']]

        # Use Django's Paginator to handle pagination
        paginator = Paginator(os_data, 2)  # 2 articles per page
        page_number = request.GET.get('page') or 1  # Default to the first page
        page_obj = paginator.get_page(page_number)

    return render(request, 'Related.html', {'page_obj': page_obj, 'os_data': os_data})

def RemoveDupl():
    connection = pymysql.connect(host='localhost',
                             user='root',
                             password = os.environ.get('EXAPWD', 'exasol'),
                             database='CV',
                             cursorclass=pymysql.cursors.DictCursor)

    try:
        with connection.cursor() as cursor:
            # Fetch all records with duplicate reference_number and title
            sql = """
            SELECT *
            FROM CV.cves
            WHERE (reference_number, title) IN (
                SELECT reference_number, title
                FROM CV.cves
                GROUP BY reference_number, title
                HAVING COUNT(*) > 1
            )
            ORDER BY reference_number, title;
            """
            cursor.execute(sql)
            results = cursor.fetchall()

            # Group records by (reference_number, title)
            duplicates = {}
            for record in results:
                key = (record['reference_number'], record['title'])
                if key not in duplicates:
                    duplicates[key] = []
                duplicates[key].append(record)

            # Delete one row per duplicate group
            for key, rows in duplicates.items():
                if len(rows) > 1:
                    # Print rows with duplicate reference_number
                    # print(f"Rows with reference_number {key[0]} and title {key[1]}: {rows}")
                    
                    # Remove one duplicate
                    row_to_remove = rows[0]  # Remove the first row for simplicity
                    query = "DELETE FROM CV.cves WHERE id = %s"
                    cursor.execute(query, (row_to_remove['id'],))
                    # print(f"Removed row with ID: {row_to_remove['id']}")
                    connection.commit()

    finally:
        connection.close()


def run_query(query, params=None):
    with connection.cursor() as cursor:
        cursor.execute(query, params)
        if query.strip().upper().startswith('SELECT'):
            return cursor.fetchall()
        else:
            return cursor.rowcount  # Number of rows affected

def manage_software(request, id=None):
    if request.method == 'POST':
        if id:  # Update existing record
            query = '''
                UPDATE Software
                SET name = %s, version = %s, build_number = %s, editor = %s
                WHERE id = %s
            '''
            params = [request.POST['name'], request.POST['version'], request.POST['build_number'], request.POST['editor'], id]
            run_query(query, params)
        else:  # Add new record
            query = '''
                INSERT INTO Software (name, version, build_number, editor)
                VALUES (%s, %s, %s, %s)
            '''
            params = [request.POST['name'], request.POST['version'], request.POST['build_number'], request.POST['editor']]
            run_query(query, params)
        return redirect('manage_software')

    elif request.method == 'DELETE' and id:
        query = 'DELETE FROM Software WHERE id = %s'
        run_query(query, [id])
        return HttpResponse(status=204)

    # GET request to render form for adding or editing
    elif request.method == 'GET':
        if id:  # Editing an existing record
            query = 'SELECT * FROM Software WHERE id = %s'
            software = get_object_or_404(run_query(query, [id]))
            context = {
                'software_list': run_query('SELECT * FROM Software'),
                'editing': True,
                'form': {
                    'name': software[1],
                    'version': software[2],
                    'build_number': software[3],
                    'editor': software[4],
                }
            }
        else:  # Adding a new record
            context = {
                'software_list': run_query('SELECT * FROM Software'),
                'editing': False,
                'form': {
                    'name': '',
                    'version': '',
                    'build_number': '',
                    'editor': '',
                }
            }
        return render(request, 'manage_software.html', context)

    # Default case, render the list view
    software_list = run_query('SELECT * FROM Software')
    context = {
        'software_list': software_list,
        'editing': False,
        'form': {
            'name': '',
            'version': '',
            'build_number': '',
            'editor': '',
        }
    }
    return render(request, 'manage_software.html', context)


class SoftwareListView(ListView):
    model = Software
    template_name = 'manage_software.html'
    context_object_name = 'software_list'

class SoftwareCreateView(CreateView):
    model = Software
    template_name = 'manage_software_form.html'
    fields = ['name', 'version', 'build_number', 'editor']
    success_url = reverse_lazy('manage_software')

class SoftwareUpdateView(UpdateView):
    model = Software
    template_name = 'manage_software_form.html'
    fields = ['name', 'version', 'build_number', 'editor']
    success_url = reverse_lazy('manage_software')

class SoftwareDeleteView(DeleteView):
    model = Software
    success_url = reverse_lazy('manage_software')